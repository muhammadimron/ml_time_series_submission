# -*- coding: utf-8 -*-
"""Time Series Submission Muhammad Imron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10uGQBjnlcAdaASLwunQ26uvdOGFLGmIi

Hallo, Kak Reviewer. 

Perkenalkan, namaku Muhammad Imron, bisa dipanggil Imron. Di submission Time Series ini, aku pakai dataset dari UCI Machine Learning namanya Tetuan City power consumption.csv. Rencananya di submission ini aku bakalan nerapin time series temperature.

Mohon bantuannya kak, untuk direview.

Terima kasih

---

https://archive.ics.uci.edu/ml/datasets/Power+consumption+of+Tetouan+city

**Import Library**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from keras.callbacks import EarlyStopping
from keras.layers import Dense, LSTM
from sklearn.model_selection import train_test_split

"""**Dataset Exploration**"""

df = pd.read_csv('/content/drive/MyDrive/Datasets/Tetuan City power consumption.csv')
df.tail()

df.describe()

df.isnull().sum()

"""**Minimal MAE yang harus dicapai**"""

treshold = (df['Temperature'].max() - df['Temperature'].min()) * 10/100
print('Minimal MAE yang harus dicapai : ', treshold)

dates = df['DateTime'].values
temp = df['Temperature'].values

plt.figure(figsize=(15,5))
plt.plot(dates[:2000], temp[:2000])
plt.title('Temperature Average',
          fontsize=20)

"""**Preprocessing**"""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

train_x, val_x, train_y, val_y = train_test_split(dates, temp, test_size=0.2, shuffle=False)

train_set = windowed_dataset(train_y, window_size=60, batch_size=100, shuffle_buffer=1000)
val_set = windowed_dataset(val_y, window_size=60, batch_size=100, shuffle_buffer=1000)

"""**Arsitektur Model**"""

model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(60, return_sequences=True),
    tf.keras.layers.LSTM(60),
    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1)
])

"""**Callbacks**"""

early_stopping = EarlyStopping(monitor='val_loss', mode='min',
                               verbose=1, patience=3)

callbacks = [early_stopping]

"""**Model Compilling and Fitting**"""

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=['mae'])
history = model.fit(train_set, epochs=100, validation_data=val_set, callbacks=callbacks)

score = model.evaluate(val_set, verbose=1)

print('Model telah berhasil mencapai minimal MAE : {} < {}'.format(score[1], treshold))

"""**Grafik Loss dan MAE**"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()